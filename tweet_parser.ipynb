{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Bitcoin\n",
    "@CoinMarketCap\n",
    "@elonmusk\n",
    "@CoinDesk\n",
    "@CoinDeskMarkets\n",
    "@CoinDeskData\n",
    "@coinbase\n",
    "@CoinbaseSupport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prs_twtr(path):\n",
    "    with open(path, 'r') as f:\n",
    "        contents = f.read()\n",
    "        page_soup = BeautifulSoup(contents, 'html.parser')\n",
    "\n",
    "    # finds each product from the store page\n",
    "    containers = page_soup.findAll(\"article\")\n",
    "\n",
    "    lista_tweet = []\n",
    "    lista_data = []\n",
    "    lista_reply = []\n",
    "    lista_commenti = []\n",
    "    lista_retweet = []\n",
    "    lista_like = []\n",
    "\n",
    "    for cnt in containers:\n",
    "        # tweet\n",
    "        try:\n",
    "            tweet = cnt.findAll(\"div\", {\"class\": \"css-901oao r-jwli3a r-1qd0xha r-a023e6 r-16dba41 r-ad9z0x r-bcqeeo r-bnwqim r-qvutc0\"}) \n",
    "            \n",
    "            try:\n",
    "                ls = tweet[0].findAll(\"span\")\n",
    "            except LookupError:\n",
    "                ls = \"****\"\n",
    "\n",
    "            tmp1 = []\n",
    "            \n",
    "            for i in range(len(ls)):\n",
    "                try:\n",
    "                    tmp1.append(ls[i].text)\n",
    "                except AttributeError:\n",
    "                    tmp1.append(ls)\n",
    "\n",
    "            string1 = \" \".join(tmp1)\n",
    "\n",
    "            lista_tweet.append(string1)\n",
    "\n",
    "        except LookupError:\n",
    "            lista_tweet.append(\"****\")\n",
    "\n",
    "        # data del tweet\n",
    "        try:\n",
    "            data = cnt.findAll(\"time\")\n",
    "            dict = data[0].attrs\n",
    "            ls1 = str(dict.values())\n",
    "            lista_data.append(ls1[14:-8])\n",
    "\n",
    "        except LookupError:\n",
    "            lista_data.append(\"****\")\n",
    "\n",
    "        # reply\n",
    "        try:\n",
    "            reply = cnt.findAll(\"div\", {\n",
    "                                \"class\": \"css-901oao r-111h2gw r-1qd0xha r-a023e6 r-16dba41 r-ad9z0x r-bcqeeo r-qvutc0\"})\n",
    "            ls2 = reply[0].findAll(\"a\")\n",
    "            tmp = []\n",
    "            for x in range(len(ls2)):\n",
    "                tmp.append(ls2[x]['href'])\n",
    "                \n",
    "            string = \" \".join(tmp)\n",
    "            lista_reply.append(string)\n",
    "\n",
    "        except LookupError:\n",
    "            lista_reply.append(\" \")\n",
    "\n",
    "        # num commenti\n",
    "        try:\n",
    "            commenti = cnt.findAll(\n",
    "                \"div\", {\"class\": \"css-1dbjc4n r-18u37iz r-1h0z5md\"})\n",
    "            ls3 = commenti[0].span.text\n",
    "            lista_commenti.append(ls3)\n",
    "            \n",
    "        except LookupError:\n",
    "            lista_commenti.append('****')\n",
    "\n",
    "        # num retweet\n",
    "        try:\n",
    "            ls4 = commenti[1].span.text\n",
    "\n",
    "            lista_retweet.append(ls4)\n",
    "        except LookupError:\n",
    "            lista_retweet.append('****')\n",
    "\n",
    "        # num like\n",
    "        try:\n",
    "            ls5 = commenti[2].span.text\n",
    "\n",
    "            lista_like.append(ls5)\n",
    "        except LookupError:\n",
    "            lista_like.append('****')\n",
    "\n",
    "    cols = [\"tweet\", \"data\", \"reply\", \"commenti\", \"retweet\", \"like\"]\n",
    "    df = pd.DataFrame(list(zip(lista_tweet, lista_data, lista_reply, lista_commenti, lista_retweet, lista_like)),\n",
    "                      columns=cols)\n",
    "\n",
    "    return(clean_parsed(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_fishes = ['Bitcoin','CoinMarketCap','elonmusk','CoinDesk','CoinDeskMarkets','CoinDeskData','coinbase','CoinbaseSuppor']\n",
    "fish = 'coinmarketcap'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_parsed(df):\n",
    "    df['data'] = pd.to_datetime(df['data'].str.replace('T',' '))\n",
    "    df.set_index('data', drop=True, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<unknown>, line 16)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/Users/matteoprandi/miniconda3/envs/nlp/lib/python3.8/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3418\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"<ipython-input-8-ff80023d08a6>\"\u001b[0m, line \u001b[1;32m1\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    get_ipython().run_cell_magic('time', '', '\\noptions = webdriver.ChromeOptions()\\nprefs = {\\'profile.default_content_setting_values\\': { \\'images\\': 2 }}\\noptions.add_experimental_option(\\'prefs\\', prefs)\\n\\ncols = [\"tweet\",\"data\",\"reply\",\"commenti\",\"retweet\",\"like\" ]\\nfinal = pd.DataFrame([],columns=cols)\\nchrome_path = \"/Users/matteoprandi/Desktop/chromedriver\"\\nbase_path = \"/Users/matteoprandi/Desktop/tweet_test/\"\\nsource_path = base_path + \"page_source.html\"\\n\\nls_account=[\"CoinMarketCap\"]\\ndt_since=\"2020-01-01\"\\ndt_until=\"2021-01-01\"\\n\\n#for fish in big_fishes:\\n    url = \"https://twitter.com/search?q=From%3A\" + fish + \"%20since%3A\"+ dt_since +\"%20until%3A\"+ dt_until + \"&src=typed_query&f=live\"\\n\\n    driver = webdriver.Chrome(options=options, executable_path=chrome_path)\\n    driver.get(url)\\n    time.sleep(5)\\n    \\n    dummy = 0\\n\\n    while True:\\n        with open(source_path, \"w\") as f:\\n            f.write(driver.page_source)\\n\\n        df = prs_twtr(source_path)\\n        final = final.append(df)\\n\\n        last_height = driver.execute_script(\"return document.body.scrollHeight\")\\n        driver.execute_script(\"window.scrollTo(0, {})\".format(last_height + 500))\\n        time.sleep(1)\\n        new_height = driver.execute_script(\"return document.body.scrollHeight\")\\n                \\n        if (final.shape[0] - dummy) >= 500:\\n            driver.close()\\n            dt_until = str(final.index[-10].date())\\n            url = \"https://twitter.com/search?q=From%3A\" + fish + \"%20since%3A\"+ dt_since +\"%20until%3A\"+ dt_until + \"&src=typed_query&f=live\"\\n            driver = webdriver.Chrome(options=options, executable_path=chrome_path)\\n            driver.get(url)\\n            time.sleep(5)\\n            \\n            dummy = final.shape[0]\\n\\n        if last_height == new_height:\\n            time.sleep(5)\\n            last_height = driver.execute_script(\"return document.body.scrollHeight\")\\n            \\n            if last_height == new_height:\\n                with open(source_path, \"w\") as f:\\n                    f.write(driver.page_source)\\n                    \\n                df = prs_twtr(source_path)\\n                final = final.append(df)\\n                final = final.drop_duplicates(subset=\\'tweet\\', keep=\"first\")\\n                final.to_csv(base_path + str(fish) + \\'.csv\\')\\n                driver.close()\\n                break\\n')\n",
      "  File \u001b[1;32m\"/Users/matteoprandi/miniconda3/envs/nlp/lib/python3.8/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m2382\u001b[0m, in \u001b[1;35mrun_cell_magic\u001b[0m\n    result = fn(*args, **kwargs)\n",
      "  File \u001b[1;32m\"<decorator-gen-54>\"\u001b[0m, line \u001b[1;32m2\u001b[0m, in \u001b[1;35mtime\u001b[0m\n",
      "  File \u001b[1;32m\"/Users/matteoprandi/miniconda3/envs/nlp/lib/python3.8/site-packages/IPython/core/magic.py\"\u001b[0m, line \u001b[1;32m187\u001b[0m, in \u001b[1;35m<lambda>\u001b[0m\n    call = lambda f, *a, **k: f(*a, **k)\n",
      "  File \u001b[1;32m\"/Users/matteoprandi/miniconda3/envs/nlp/lib/python3.8/site-packages/IPython/core/magics/execution.py\"\u001b[0m, line \u001b[1;32m1277\u001b[0m, in \u001b[1;35mtime\u001b[0m\n    expr_ast = self.shell.compile.ast_parse(expr)\n",
      "\u001b[0;36m  File \u001b[0;32m\"/Users/matteoprandi/miniconda3/envs/nlp/lib/python3.8/site-packages/IPython/core/compilerop.py\"\u001b[0;36m, line \u001b[0;32m101\u001b[0;36m, in \u001b[0;35mast_parse\u001b[0;36m\u001b[0m\n\u001b[0;31m    return compile(source, filename, symbol, self.flags | PyCF_ONLY_AST, 1)\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"<unknown>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    url = \"https://twitter.com/search?q=From%3A\" + fish + \"%20since%3A\"+ dt_since +\"%20until%3A\"+ dt_until + \"&src=typed_query&f=live\"\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "prefs = {'profile.default_content_setting_values': { 'images': 2 }}\n",
    "options.add_experimental_option('prefs', prefs)\n",
    "\n",
    "cols = [\"tweet\",\"data\",\"reply\",\"commenti\",\"retweet\",\"like\" ]\n",
    "final = pd.DataFrame([],columns=cols)\n",
    "chrome_path = \"/Users/matteoprandi/Desktop/chromedriver\"\n",
    "base_path = \"/Users/matteoprandi/Desktop/tweet_test/\"\n",
    "source_path = base_path + \"page_source.html\"\n",
    "\n",
    "ls_account=[\"CoinMarketCap\"]\n",
    "dt_since=\"2020-01-01\"\n",
    "dt_until=\"2021-01-01\"\n",
    "\n",
    "for fish in big_fishes:\n",
    "    url = \"https://twitter.com/search?q=From%3A\" + fish + \"%20since%3A\"+ dt_since +\"%20until%3A\"+ dt_until + \"&src=typed_query&f=live\"\n",
    "\n",
    "    driver = webdriver.Chrome(options=options, executable_path=chrome_path)\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    dummy = 0\n",
    "\n",
    "    while True:\n",
    "        with open(source_path, \"w\") as f:\n",
    "            f.write(driver.page_source)\n",
    "\n",
    "        df = prs_twtr(source_path)\n",
    "        final = final.append(df)\n",
    "\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        driver.execute_script(\"window.scrollTo(0, {})\".format(last_height + 500))\n",
    "        time.sleep(1)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                \n",
    "        if (final.shape[0] - dummy) >= 500:\n",
    "            driver.close()\n",
    "            dt_until = str(final.index[-10].date())\n",
    "            url = \"https://twitter.com/search?q=From%3A\" + fish + \"%20since%3A\"+ dt_since +\"%20until%3A\"+ dt_until + \"&src=typed_query&f=live\"\n",
    "            driver = webdriver.Chrome(options=options, executable_path=chrome_path)\n",
    "            driver.get(url)\n",
    "            time.sleep(5)\n",
    "            \n",
    "            dummy = final.shape[0]\n",
    "\n",
    "        if last_height == new_height:\n",
    "            time.sleep(5)\n",
    "            last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            \n",
    "            if last_height == new_height:\n",
    "                with open(source_path, \"w\") as f:\n",
    "                    f.write(driver.page_source)\n",
    "                    \n",
    "                df = prs_twtr(source_path)\n",
    "                final = final.append(df)\n",
    "                final = final.drop_duplicates(subset='tweet', keep=\"first\")\n",
    "                final.to_csv(base_path + str(fish) + '.csv')\n",
    "                driver.close()\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "prefs = {'profile.default_content_setting_values': { 'images': 2 }}\n",
    "options.add_experimental_option('prefs', prefs)\n",
    "\n",
    "cols = [\"tweet\",\"data\",\"reply\",\"commenti\",\"retweet\",\"like\" ]\n",
    "final = pd.DataFrame([],columns=cols)\n",
    "chrome_path = \"/Users/matteoprandi/Desktop/chromedriver\"\n",
    "base_path = \"/Users/matteoprandi/Desktop/tweet_test/\"\n",
    "source_path = base_path + \"page_source.html\"\n",
    "\n",
    "ls_account=[\"CoinMarketCap\"]\n",
    "dt_since=\"2020-01-01\"\n",
    "dt_until=\"2021-01-01\"\n",
    "\n",
    "fish = 'coinmarketcap'\n",
    "\n",
    "url = \"https://twitter.com/search?q=From%3A\" + fish + \"%20since%3A\"+ dt_since +\"%20until%3A\"+ dt_until + \"&src=typed_query&f=live\"\n",
    "\n",
    "driver = webdriver.Chrome(options=options, executable_path=chrome_path)\n",
    "driver.get(url)\n",
    "time.sleep(5)\n",
    "\n",
    "dummy = 0\n",
    "\n",
    "while True:\n",
    "    with open(source_path, \"w\") as f:\n",
    "        f.write(driver.page_source)\n",
    "\n",
    "    df = prs_twtr(source_path)\n",
    "    final = final.append(df)\n",
    "\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    driver.execute_script(\"window.scrollTo(0, {})\".format(last_height + 500))\n",
    "    time.sleep(1)\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    if (final.shape[0] - dummy) >= 500:\n",
    "        driver.close()\n",
    "        dt_until = str(final.index[-10].date())\n",
    "        url = \"https://twitter.com/search?q=From%3A\" + fish + \"%20since%3A\"+ dt_since +\"%20until%3A\"+ dt_until + \"&src=typed_query&f=live\"\n",
    "        driver = webdriver.Chrome(options=options, executable_path=chrome_path)\n",
    "        driver.get(url)\n",
    "        time.sleep(5)\n",
    "\n",
    "        dummy = final.shape[0]\n",
    "\n",
    "    if last_height == new_height:\n",
    "        time.sleep(5)\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        if last_height == new_height:\n",
    "            with open(source_path, \"w\") as f:\n",
    "                f.write(driver.page_source)\n",
    "\n",
    "            df = prs_twtr(source_path)\n",
    "            final = final.append(df)\n",
    "            final = final.drop_duplicates(subset='tweet', keep=\"first\")\n",
    "            final.to_csv(base_path + str(fish) + '.csv')\n",
    "            driver.close()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3478, 7)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = pd.read_csv(base_path + 'CoinMarketCap.csv')\n",
    "t.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
